{"cells":[{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7381,"status":"ok","timestamp":1682593363876,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"V4pXwlAwwEK-","outputId":"07c867ef-7818-4e79-dd24-086db6c0e701"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-04-27 11:02:40--  https://pjreddie.com/media/files/yolov3.weights\n","Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n","Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 248007048 (237M) [application/octet-stream]\n","Saving to: ‘yolov3.weights.1’\n","\n","yolov3.weights.1    100%[===================\u003e] 236.52M  39.8MB/s    in 6.3s    \n","\n","2023-04-27 11:02:46 (37.4 MB/s) - ‘yolov3.weights.1’ saved [248007048/248007048]\n","\n"]}],"source":["!wget https://pjreddie.com/media/files/yolov3.weights"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1682593363877,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"BSpq2IpUgeHN"},"outputs":[],"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import numpy as np\n","import PIL.Image, PIL.ImageDraw, PIL.ImageFont\n","from IPython.display import display\n","from seaborn import color_palette\n","import cv2"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1682593363877,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"YfKxKNehlyqz"},"outputs":[],"source":["_BATCH_NORM_DECAY = 0.9\n","_BATCH_NORM_EPSILON = 1e-05\n","_LEAKY_RELU = 0.1\n","_ANCHORS = [(10, 13), (16, 30), (33, 23),\n","            (30, 61), (62, 45), (59, 119),\n","            (116, 90), (156, 198), (373, 326)]\n","_MODEL_SIZE = (416, 416)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1682593363878,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"Et9OcbLul5kb"},"outputs":[],"source":["def batch_norm(inputs, training, data_format):\n","    \"\"\"Performs a batch normalization using a standard set of parameters.\"\"\"\n","    return tf.layers.batch_normalization(\n","        inputs=inputs, axis=1 if data_format == 'channels_first' else 3,\n","        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON,\n","        scale=True, training=training)\n","\n","\n","def fixed_padding(inputs, kernel_size, data_format):\n","    \"\"\"ResNet implementation of fixed padding.\n","\n","    Pads the input along the spatial dimensions independently of input size.\n","\n","    Args:\n","        inputs: Tensor input to be padded.\n","        kernel_size: The kernel to be used in the conv2d or max_pool2d.\n","        data_format: The input format.\n","    Returns:\n","        A tensor with the same format as the input.\n","    \"\"\"\n","    pad_total = kernel_size - 1\n","    pad_beg = pad_total // 2\n","    pad_end = pad_total - pad_beg\n","\n","    if data_format == 'channels_first':\n","        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n","                                        [pad_beg, pad_end],\n","                                        [pad_beg, pad_end]])\n","    else:\n","        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n","                                        [pad_beg, pad_end], [0, 0]])\n","    return padded_inputs\n","\n","\n","def conv2d_fixed_padding(inputs, filters, kernel_size, data_format, strides=1):\n","    \"\"\"Strided 2-D convolution with explicit padding.\"\"\"\n","    if strides \u003e 1:\n","        inputs = fixed_padding(inputs, kernel_size, data_format)\n","\n","    return tf.layers.conv2d(\n","        inputs=inputs, filters=filters, kernel_size=kernel_size,\n","        strides=strides, padding=('SAME' if strides == 1 else 'VALID'),\n","        use_bias=False, data_format=data_format)"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1682593363878,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"oY5ODip7l6z3"},"outputs":[],"source":["def darknet53_residual_block(inputs, filters, training, data_format,\n","                             strides=1):\n","    \"\"\"Creates a residual block for Darknet.\"\"\"\n","    shortcut = inputs\n","\n","    inputs = conv2d_fixed_padding(\n","        inputs, filters=filters, kernel_size=1, strides=strides,\n","        data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","\n","    inputs = conv2d_fixed_padding(\n","        inputs, filters=2 * filters, kernel_size=3, strides=strides,\n","        data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","\n","    inputs += shortcut\n","\n","    return inputs\n","\n","\n","def darknet53(inputs, training, data_format):\n","    \"\"\"Creates Darknet53 model for feature extraction.\"\"\"\n","    inputs = conv2d_fixed_padding(inputs, filters=32, kernel_size=3,\n","                                  data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","    inputs = conv2d_fixed_padding(inputs, filters=64, kernel_size=3,\n","                                  strides=2, data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","\n","    inputs = darknet53_residual_block(inputs, filters=32, training=training,\n","                                      data_format=data_format)\n","\n","    inputs = conv2d_fixed_padding(inputs, filters=128, kernel_size=3,\n","                                  strides=2, data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","\n","    for _ in range(2):\n","        inputs = darknet53_residual_block(inputs, filters=64,\n","                                          training=training,\n","                                          data_format=data_format)\n","\n","    inputs = conv2d_fixed_padding(inputs, filters=256, kernel_size=3,\n","                                  strides=2, data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","\n","    for _ in range(8):\n","        inputs = darknet53_residual_block(inputs, filters=128,\n","                                          training=training,\n","                                          data_format=data_format)\n","\n","    route1 = inputs\n","\n","    inputs = conv2d_fixed_padding(inputs, filters=512, kernel_size=3,\n","                                  strides=2, data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","\n","    for _ in range(8):\n","        inputs = darknet53_residual_block(inputs, filters=256,\n","                                          training=training,\n","                                          data_format=data_format)\n","\n","    route2 = inputs\n","\n","    inputs = conv2d_fixed_padding(inputs, filters=1024, kernel_size=3,\n","                                  strides=2, data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","\n","    for _ in range(4):\n","        inputs = darknet53_residual_block(inputs, filters=512,\n","                                          training=training,\n","                                          data_format=data_format)\n","\n","    return route1, route2, inputs"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1682593363878,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"6ViAcbO0mArW"},"outputs":[],"source":["def yolo_convolution_block(inputs, filters, training, data_format):\n","    \"\"\"Creates convolution operations layer used after Darknet.\"\"\"\n","    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n","                                  data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","\n","    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n","                                  data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","\n","    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n","                                  data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","\n","    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n","                                  data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","\n","    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n","                                  data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","\n","    route = inputs\n","\n","    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n","                                  data_format=data_format)\n","    inputs = batch_norm(inputs, training=training, data_format=data_format)\n","    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","\n","    return route, inputs"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1682593363879,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"s_u1WcehmEaU"},"outputs":[],"source":["def yolo_layer(inputs, n_classes, anchors, img_size, data_format):\n","    \"\"\"Creates Yolo final detection layer.\n","\n","    Detects boxes with respect to anchors.\n","\n","    Args:\n","        inputs: Tensor input.\n","        n_classes: Number of labels.\n","        anchors: A list of anchor sizes.\n","        img_size: The input size of the model.\n","        data_format: The input format.\n","\n","    Returns:\n","        Tensor output.\n","    \"\"\"\n","    n_anchors = len(anchors)\n","\n","    inputs = tf.layers.conv2d(inputs, filters=n_anchors * (5 + n_classes),\n","                              kernel_size=1, strides=1, use_bias=True,\n","                              data_format=data_format)\n","\n","    shape = inputs.get_shape().as_list()\n","    grid_shape = shape[2:4] if data_format == 'channels_first' else shape[1:3]\n","    if data_format == 'channels_first':\n","        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n","    inputs = tf.reshape(inputs, [-1, n_anchors * grid_shape[0] * grid_shape[1],\n","                                 5 + n_classes])\n","\n","    strides = (img_size[0] // grid_shape[0], img_size[1] // grid_shape[1])\n","\n","    box_centers, box_shapes, confidence, classes = \\\n","        tf.split(inputs, [2, 2, 1, n_classes], axis=-1)\n","\n","    x = tf.range(grid_shape[0], dtype=tf.float32)\n","    y = tf.range(grid_shape[1], dtype=tf.float32)\n","    x_offset, y_offset = tf.meshgrid(x, y)\n","    x_offset = tf.reshape(x_offset, (-1, 1))\n","    y_offset = tf.reshape(y_offset, (-1, 1))\n","    x_y_offset = tf.concat([x_offset, y_offset], axis=-1)\n","    x_y_offset = tf.tile(x_y_offset, [1, n_anchors])\n","    x_y_offset = tf.reshape(x_y_offset, [1, -1, 2])\n","    box_centers = tf.nn.sigmoid(box_centers)\n","    box_centers = (box_centers + x_y_offset) * strides\n","\n","    anchors = tf.tile(anchors, [grid_shape[0] * grid_shape[1], 1])\n","    box_shapes = tf.exp(box_shapes) * tf.to_float(anchors)\n","\n","    confidence = tf.nn.sigmoid(confidence)\n","\n","    classes = tf.nn.sigmoid(classes)\n","\n","    inputs = tf.concat([box_centers, box_shapes,\n","                        confidence, classes], axis=-1)\n","\n","    return inputs"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1682593363879,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"1VsSdsyXmMRz"},"outputs":[],"source":["def upsample(inputs, out_shape, data_format):\n","    \"\"\"Upsamples to `out_shape` using nearest neighbor interpolation.\"\"\"\n","    if data_format == 'channels_first':\n","        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n","        new_height = out_shape[3]\n","        new_width = out_shape[2]\n","    else:\n","        new_height = out_shape[2]\n","        new_width = out_shape[1]\n","\n","    inputs = tf.image.resize_nearest_neighbor(inputs, (new_height, new_width))\n","\n","    if data_format == 'channels_first':\n","        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n","\n","    return inputs"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1682593363880,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"ahuasSldmOWP"},"outputs":[],"source":["def build_boxes(inputs):\n","    \"\"\"Computes top left and bottom right points of the boxes.\"\"\"\n","    center_x, center_y, width, height, confidence, classes = \\\n","        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)\n","\n","    top_left_x = center_x - width / 2\n","    top_left_y = center_y - height / 2\n","    bottom_right_x = center_x + width / 2\n","    bottom_right_y = center_y + height / 2\n","\n","    boxes = tf.concat([top_left_x, top_left_y,\n","                       bottom_right_x, bottom_right_y,\n","                       confidence, classes], axis=-1)\n","\n","    return boxes\n","\n","\n","def non_max_suppression(inputs, n_classes, max_output_size, iou_threshold,\n","                        confidence_threshold):\n","    \"\"\"Performs non-max suppression separately for each class.\n","\n","    Args:\n","        inputs: Tensor input.\n","        n_classes: Number of classes.\n","        max_output_size: Max number of boxes to be selected for each class.\n","        iou_threshold: Threshold for the IOU.\n","        confidence_threshold: Threshold for the confidence score.\n","    Returns:\n","        A list containing class-to-boxes dictionaries\n","            for each sample in the batch.\n","    \"\"\"\n","    batch = tf.unstack(inputs)\n","    boxes_dicts = []\n","    for boxes in batch:\n","        boxes = tf.boolean_mask(boxes, boxes[:, 4] \u003e confidence_threshold)\n","        classes = tf.argmax(boxes[:, 5:], axis=-1)\n","        classes = tf.expand_dims(tf.to_float(classes), axis=-1)\n","        boxes = tf.concat([boxes[:, :5], classes], axis=-1)\n","\n","        boxes_dict = dict()\n","        for cls in range(n_classes):\n","            mask = tf.equal(boxes[:, 5], cls)\n","            mask_shape = mask.get_shape()\n","            if mask_shape.ndims != 0:\n","                class_boxes = tf.boolean_mask(boxes, mask)\n","                boxes_coords, boxes_conf_scores, _ = tf.split(class_boxes,\n","                                                              [4, 1, -1],\n","                                                              axis=-1)\n","                boxes_conf_scores = tf.reshape(boxes_conf_scores, [-1])\n","                indices = tf.image.non_max_suppression(boxes_coords,\n","                                                       boxes_conf_scores,\n","                                                       max_output_size,\n","                                                       iou_threshold)\n","                class_boxes = tf.gather(class_boxes, indices)\n","                boxes_dict[cls] = class_boxes[:, :5]\n","\n","        boxes_dicts.append(boxes_dict)\n","\n","    return boxes_dicts"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1682593363880,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"ZtrPgDhxmRof"},"outputs":[],"source":["class Yolo_v3:\n","    \"\"\"Yolo v3 model class.\"\"\"\n","\n","    def __init__(self, n_classes, model_size, max_output_size, iou_threshold,\n","                 confidence_threshold, data_format=None):\n","        \"\"\"Creates the model.\n","\n","        Args:\n","            n_classes: Number of class labels.\n","            model_size: The input size of the model.\n","            max_output_size: Max number of boxes to be selected for each class.\n","            iou_threshold: Threshold for the IOU.\n","            confidence_threshold: Threshold for the confidence score.\n","            data_format: The input format.\n","\n","        Returns:\n","            None.\n","        \"\"\"\n","        if not data_format:\n","            if tf.test.is_built_with_cuda():\n","                data_format = 'channels_first'\n","            else:\n","                data_format = 'channels_last'\n","\n","        self.n_classes = n_classes\n","        self.model_size = model_size\n","        self.max_output_size = max_output_size\n","        self.iou_threshold = iou_threshold\n","        self.confidence_threshold = confidence_threshold\n","        self.data_format = data_format\n","\n","    def __call__(self, inputs, training):\n","        \"\"\"Add operations to detect boxes for a batch of input images.\n","\n","        Args:\n","            inputs: A Tensor representing a batch of input images.\n","            training: A boolean, whether to use in training or inference mode.\n","\n","        Returns:\n","            A list containing class-to-boxes dictionaries\n","                for each sample in the batch.\n","        \"\"\"\n","        with tf.variable_scope('yolo_v3_model'):\n","            if self.data_format == 'channels_first':\n","                inputs = tf.transpose(inputs, [0, 3, 1, 2])\n","\n","            inputs = inputs / 255\n","\n","            route1, route2, inputs = darknet53(inputs, training=training,\n","                                               data_format=self.data_format)\n","\n","            route, inputs = yolo_convolution_block(\n","                inputs, filters=512, training=training,\n","                data_format=self.data_format)\n","            detect1 = yolo_layer(inputs, n_classes=self.n_classes,\n","                                 anchors=_ANCHORS[6:9],\n","                                 img_size=self.model_size,\n","                                 data_format=self.data_format)\n","\n","            inputs = conv2d_fixed_padding(route, filters=256, kernel_size=1,\n","                                          data_format=self.data_format)\n","            inputs = batch_norm(inputs, training=training,\n","                                data_format=self.data_format)\n","            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","            upsample_size = route2.get_shape().as_list()\n","            inputs = upsample(inputs, out_shape=upsample_size,\n","                              data_format=self.data_format)\n","            axis = 1 if self.data_format == 'channels_first' else 3\n","            inputs = tf.concat([inputs, route2], axis=axis)\n","            route, inputs = yolo_convolution_block(\n","                inputs, filters=256, training=training,\n","                data_format=self.data_format)\n","            detect2 = yolo_layer(inputs, n_classes=self.n_classes,\n","                                 anchors=_ANCHORS[3:6],\n","                                 img_size=self.model_size,\n","                                 data_format=self.data_format)\n","\n","            inputs = conv2d_fixed_padding(route, filters=128, kernel_size=1,\n","                                          data_format=self.data_format)\n","            inputs = batch_norm(inputs, training=training,\n","                                data_format=self.data_format)\n","            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n","            upsample_size = route1.get_shape().as_list()\n","            inputs = upsample(inputs, out_shape=upsample_size,\n","                              data_format=self.data_format)\n","            inputs = tf.concat([inputs, route1], axis=axis)\n","            route, inputs = yolo_convolution_block(\n","                inputs, filters=128, training=training,\n","                data_format=self.data_format)\n","            detect3 = yolo_layer(inputs, n_classes=self.n_classes,\n","                                 anchors=_ANCHORS[0:3],\n","                                 img_size=self.model_size,\n","                                 data_format=self.data_format)\n","\n","            inputs = tf.concat([detect1, detect2, detect3], axis=1)\n","\n","            inputs = build_boxes(inputs)\n","\n","            boxes_dicts = non_max_suppression(\n","                inputs, n_classes=self.n_classes,\n","                max_output_size=self.max_output_size,\n","                iou_threshold=self.iou_threshold,\n","                confidence_threshold=self.confidence_threshold)\n","\n","            return boxes_dicts"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1682593363881,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"rNlaPSx4mZYB"},"outputs":[],"source":["def load_images(img_names, model_size):\n","    \"\"\"Loads images in a 4D array.\n","\n","    Args:\n","        img_names: A list of images names.\n","        model_size: The input size of the model.\n","        data_format: A format for the array returned\n","            ('channels_first' or 'channels_last').\n","\n","    Returns:\n","        A 4D NumPy array.\n","    \"\"\"\n","    imgs = []\n","\n","    for img_name in img_names:\n","        img = PIL.Image.open(img_name)\n","        img = img.resize(size=model_size)\n","        img = np.array(img, dtype=np.float32)\n","        img = np.expand_dims(img, axis=0)\n","        imgs.append(img)\n","\n","    imgs = np.concatenate(imgs)\n","\n","    return imgs\n","\n","\n","def load_class_names(file_name):\n","    \"\"\"Returns a list of class names read from `file_name`.\"\"\"\n","    with open(file_name, 'r') as f:\n","        class_names = f.read().splitlines()\n","    return class_names\n","\n","\n","def draw_boxes(img_names, boxes_dicts, class_names, model_size):\n","    \"\"\"Draws detected boxes.\n","\n","    Args:\n","        img_names: A list of input images names.\n","        boxes_dict: A class-to-boxes dictionary.\n","        class_names: A class names list.\n","        model_size: The input size of the model.\n","\n","    Returns:\n","        None.\n","    \"\"\"\n","    colors = ((np.array(color_palette(\"hls\", 80)) * 255)).astype(np.uint8)\n","    for num, img_name, boxes_dict in zip(range(len(img_names)), img_names,\n","                                         boxes_dicts):\n","        img = PIL.Image.open(img_name)\n","        draw = PIL.ImageDraw.Draw(img)\n","        font = PIL.ImageFont.load_default()\n","        resize_factor = \\\n","            (img.size[0] / model_size[0], img.size[1] / model_size[1])\n","        for cls in range(len(class_names)):\n","            boxes = boxes_dict[cls]\n","            if np.size(boxes) != 0:\n","                color = colors[cls]\n","                for box in boxes:\n","                    xy, confidence = box[:4], box[4]\n","                    xy = [xy[i] * resize_factor[i % 2] for i in range(4)]\n","                    x0, y0 = xy[0], xy[1]\n","                    thickness = (img.size[0] + img.size[1]) // 200\n","                    for t in np.linspace(0, 1, thickness):\n","                        xy[0], xy[1] = xy[0] + t, xy[1] + t\n","                        xy[2], xy[3] = xy[2] - t, xy[3] - t\n","                        draw.rectangle(xy, outline=tuple(color))\n","                    text = '{} {:.1f}%'.format(class_names[cls],\n","                                               confidence * 100)\n","                    text_size = draw.textsize(text, font=font)\n","                    draw.rectangle(\n","                        [x0, y0 - text_size[1], x0 + text_size[0], y0],\n","                        fill=tuple(color))\n","                    draw.text((x0, y0 - text_size[1]), text, fill='black',\n","                              font=font)\n","\n","        display(img)"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1682593363882,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"kfpbzNTPmdvZ"},"outputs":[],"source":["\n","\n","def load_weights(variables, file_name):\n","    \"\"\"Reshapes and loads official pretrained Yolo weights.\n","\n","    Args:\n","        variables: A list of tf.Variable to be assigned.\n","        file_name: A name of a file containing weights.\n","\n","    Returns:\n","        A list of assign operations.\n","    \"\"\"\n","    with open(file_name, \"rb\") as f:\n","        # Skip first 5 values containing irrelevant info\n","        np.fromfile(f, dtype=np.int32, count=5)\n","        weights = np.fromfile(f, dtype=np.float32)\n","\n","        assign_ops = []\n","        ptr = 0\n","\n","        # Load weights for Darknet part.\n","        # Each convolution layer has batch normalization.\n","        for i in range(52):\n","            conv_var = variables[5 * i]\n","            gamma, beta, mean, variance = variables[5 * i + 1:5 * i + 5]\n","            batch_norm_vars = [beta, gamma, mean, variance]\n","\n","            for var in batch_norm_vars:\n","                shape = var.shape.as_list()\n","                num_params = np.prod(shape)\n","                var_weights = weights[ptr:ptr + num_params].reshape(shape)\n","                ptr += num_params\n","                assign_ops.append(tf.assign(var, var_weights))\n","\n","            shape = conv_var.shape.as_list()\n","            num_params = np.prod(shape)\n","            var_weights = weights[ptr:ptr + num_params].reshape(\n","                (shape[3], shape[2], shape[0], shape[1]))\n","            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n","            ptr += num_params\n","            assign_ops.append(tf.assign(conv_var, var_weights))\n","\n","        # Loading weights for Yolo part.\n","        # 7th, 15th and 23rd convolution layer has biases and no batch norm.\n","        ranges = [range(0, 6), range(6, 13), range(13, 20)]\n","        unnormalized = [6, 13, 20]\n","        for j in range(3):\n","            for i in ranges[j]:\n","                current = 52 * 5 + 5 * i + j * 2\n","                conv_var = variables[current]\n","                gamma, beta, mean, variance =  \\\n","                    variables[current + 1:current + 5]\n","                batch_norm_vars = [beta, gamma, mean, variance]\n","\n","                for var in batch_norm_vars:\n","                    shape = var.shape.as_list()\n","                    num_params = np.prod(shape)\n","                    var_weights = weights[ptr:ptr + num_params].reshape(shape)\n","                    ptr += num_params\n","                    assign_ops.append(tf.assign(var, var_weights))\n","\n","                shape = conv_var.shape.as_list()\n","                num_params = np.prod(shape)\n","                var_weights = weights[ptr:ptr + num_params].reshape(\n","                    (shape[3], shape[2], shape[0], shape[1]))\n","                var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n","                ptr += num_params\n","                assign_ops.append(tf.assign(conv_var, var_weights))\n","\n","            bias = variables[52 * 5 + unnormalized[j] * 5 + j * 2 + 1]\n","            shape = bias.shape.as_list()\n","            num_params = np.prod(shape)\n","            var_weights = weights[ptr:ptr + num_params].reshape(shape)\n","            ptr += num_params\n","            assign_ops.append(tf.assign(bias, var_weights))\n","\n","            conv_var = variables[52 * 5 + unnormalized[j] * 5 + j * 2]\n","            shape = conv_var.shape.as_list()\n","            num_params = np.prod(shape)\n","            var_weights = weights[ptr:ptr + num_params].reshape(\n","                (shape[3], shape[2], shape[0], shape[1]))\n","            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n","            ptr += num_params\n","            assign_ops.append(tf.assign(conv_var, var_weights))\n","\n","    return assign_ops"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1682593363883,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"37JPi7HHmg75"},"outputs":[],"source":["# import os\n","# import tensorflow_datasets as tfds\n","# import pathlib\n","# dataset_url='https://data.vision.ee.ethz.ch/cvl/aess/cvpr2008/seq03-img-left.tar.gz'\n","# data_direc=tf.keras.utils.get_file(origin=dataset_url,fname='temp',untar=True)\n","# data_direc=pathlib.Path(data_direc)\n","# !wget https://data.vision.ee.ethz.ch/cvl/aess/cvpr2008/seq04-img-left.tar.gz\n"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":883,"status":"ok","timestamp":1682593365863,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"D2arNih2S2Fn"},"outputs":[],"source":["# !tar --gunzip --extract --verbose --file=seq04-img-left.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1jKYnq6SheijtIbf1YC1puSLa4CmvuQl-"},"id":"NQLHnRqRUFY6","outputId":"0ae7478d-8a08-426d-a51d-3979259a942d"},"outputs":[],"source":["# img_names = list(glob.glob('image*27_0.png'))\n","import glob\n","img_names = list(glob.glob('dataset/*.[jp][pn][jpe]g'))\n","# !ls \n","# PIL.Image.open(str(img_names[0]))\n","for img in img_names: display(PIL.Image.open(img))"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"elapsed":824,"status":"error","timestamp":1682593445185,"user":{"displayName":"Harman Gill","userId":"05241840434140621512"},"user_tz":-330},"id":"jI1kjGC2mms6","outputId":"10c3910f-e443-4a04-ac26-acbe9a962605"},"outputs":[{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-38-0b0c8dc86a3b\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 3\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_MODEL_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_class_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'coco.names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-32-0b34309943ed\u003e\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(img_names, model_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 22\u001b[0;31m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"]}],"source":["tf.reset_default_graph()\n","batch_size = len(img_names)\n","batch = load_images(img_names, model_size=_MODEL_SIZE)\n","class_names = load_class_names('coco.names')\n","n_classes = len(class_names)\n","max_output_size = 10\n","iou_threshold = 0.5\n","confidence_threshold = 0.5\n","model = Yolo_v3(n_classes=n_classes, model_size=_MODEL_SIZE,\n","                max_output_size=max_output_size,\n","                iou_threshold=iou_threshold,\n","                confidence_threshold=confidence_threshold,data_format='channels_last')\n","\n","inputs = tf.placeholder(tf.float32, [batch_size, 416, 416, 3])\n","\n","detections = model(inputs, training=False)\n","\n","model_vars = tf.global_variables(scope='yolo_v3_model')\n","assign_ops = load_weights(model_vars, 'yolov3.weights')\n","\n","with tf.Session() as sess:\n","    sess.run(assign_ops)\n","    detection_result = sess.run(detections, feed_dict={inputs: batch})\n","    \n","draw_boxes(img_names, detection_result, class_names, _MODEL_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdHjYyBG7XUQ"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPsKe/CJ7hax8Q9j1l+1Kzt","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}